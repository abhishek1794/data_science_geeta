{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#loading datasets\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test  = pd.read_csv('test.csv')\n",
    "#concatenation\n",
    "df_combined = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing train and test data\n",
    "df_drop_target=df_combined.drop(['Approved'], axis=1)\n",
    "Xtrain = df_drop_target.head(69713)\n",
    "Xtest = df_drop_target.tail(30037)\n",
    "Xtarget=df_train['Approved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(Xtrain, Xtarget, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X is independent features/ predictors\n",
    "y is target variable\n",
    "# scaling of target variable is not required \n",
    "\n",
    "ss = StandardScaler().fit(X,y)\n",
    "X_ = ss.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logreg = LogisticRegression()\\nlogreg.fit(X_train, y_train)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is not working bc data is not cleaned\n",
    "'''logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### base model\n",
    "\n",
    "model_cl = LogisticRegression()\n",
    "model_cl = model_cl.fit(X_train, y_train) #### training \n",
    "predict_train = model_cl.predict(X_test)\n",
    "print(accuracy_score(y_train, model_cl.predict(X_train))) # accuracy on train\n",
    "accuracy_score(y_test, model_cl.predict(X_test)) # accuracy on cross validation\n",
    "\n",
    "### prediction on the test.csv\n",
    "y_test2 = model_cl.predict(X_test_)\n",
    "\n",
    "# test2 is the set dataset.. not having target variable\n",
    "test2['Survived'] = y_test2\n",
    "\n",
    "### save dataframe as csv file\n",
    "# test2[['PassengerId','Survived']].to_csv('submission.csv', index = False)\n",
    "\n",
    "model_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cl = LogisticRegression()\n",
    "\n",
    "param = {'C':[0.1,1,10],\n",
    "        'penalty':['l1','l2'] }#PENALTIES ARE USED TO SHRINK THE CORRELATED FEATURES WITHOUT REMOVING IN L2\n",
    "                               # L1 PENALTY IS USED TO SHRINK THE USELESS FEATURE TO ZERO, \n",
    "                               # USED IN FEATURE ENGINEERING AND IMPORTANT FEATURE SELECTION\n",
    "# Using GRID SEARCH CV\n",
    "gs = GridSearchCV(estimator=model_cl, param_grid=param, scoring='accuracy', cv = 3,\n",
    "                 n_jobs= -1)\n",
    "\n",
    "#  Grid Search \n",
    "## fiting\n",
    "gs = gs.fit(X_train,y_train)\n",
    "gs.best_score_\n",
    "\n",
    "gs.best_params_\n",
    "\n",
    "## accuracy on validation dataset\n",
    "accuracy_score(y_test, gs.predict(X_test))\n",
    "\n",
    "pd.DataFrame(gs.cv_results_).sort_values(by = ['mean_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dtree = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=5, min_samples_leaf=5)\n",
    "dtree.fit(X_train, y_train)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Accuracy / Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for logistic regression \n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for decision tree\n",
    "y_pred_tree=dtree.predict(X_test)\n",
    "print('Accuracy of Decision tree classifier on test set: {:.2f}'.format(dtree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing confusion matrix for getting the exact number of times positive and and negatives are wrongly classified\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing classification Report for finding what is the accuracy,precision,recall of the model based on classification tree\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing ruc_auc_score and roc_curve from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "tree_roc_auc=roc_auc_score(y_test, dtree.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, dtree.predict_proba(X_test)[:,1])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot(fpr, tpr, label='Tree Regression (area = %0.2f)' % tree_roc_auc)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''As seen from the auc of both logistic regression and Decision tree \n",
    "curves are overlapping hence they are same'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTIONS FOR UNSEEN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = logreg.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction_tree=dtree.predict(Xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING PREDICTED COLUMN INTO THE TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Approved']=y_prediction\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF, pipeline, condusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('over', RandomOverSampler()), ('model', RandomForestClassifier())] ### steps\n",
    "#steps = [('under', RandomUnderSampler()), ('model', RandomForestClassifier())]\n",
    "#steps = [('over', SMOTE(random_state= 42)), ('model', RandomForestClassifier())]\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=steps) ### object\n",
    "\n",
    "pipeline.fit(X_train, y_train) ## training\n",
    "\n",
    "y_hat = pipeline.predict(X_test) ## prediction\n",
    "print('Validation set: confusion matrix with random-over-sampling')\n",
    "cm = confusion_matrix(y_test, y_hat, labels=[0,1])\n",
    "annot_kws = {\"ha\": 'left',\"va\": 'top'}\n",
    "print(cm)\n",
    "sns.heatmap(cm, annot=True, center = True,fmt=\"d\",linewidths=0.5,cmap=\"Greens\",vmin=0,vmax = cm.max(),annot_kws=annot_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using for loop\n",
    "for k in range(4,10,1):\n",
    "    steps = [('over', SMOTE(random_state=42, k_neighbors=k)), ('model', RandomForestClassifier())]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    pipeline.fit(X_train, y_train) ## training\n",
    "    y_hat = pipeline.predict(X_test) ## prediction\n",
    "    print('validation set:AUC Score: {}%'.format(round(roc_auc_score(y_test, y_hat)*100,4)))\n",
    "    print('Validation set: confusion matrix with SMOTE-over-sampling with k = {}'.format(k))\n",
    "    print(confusion_matrix(y_test, y_hat, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training AUC Score: {}'.format(round(roc_auc_score(y_train, pipeline.predict(X_train))*100,4)))\n",
    "print('validation AUC Score: {}'.format(round(roc_auc_score(y_test, y_hat)*100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  searching best k value \n",
    " - combining over-sampling and under-sampling\n",
    " - tunning k value in K_neighbours\n",
    "# values to evaluate\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "k_values = [1, 2, 3, 4, 5, 6, 7]\n",
    "for k in k_values:\n",
    "# define pipeline\n",
    "    model = RandomForestClassifier()\n",
    "    over  = SMOTE(sampling_strategy=0.1, k_neighbors=k)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "    steps = [('over', over), ('under', under), ('model', model)]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    # evaluate pipeline\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "    score = np.mean(scores)\n",
    "    print('> k=%d, Mean ROC AUC: %.3f' % (k, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('over', RandomOverSampler()), ('model', LogisticRegression())] ### steps\n",
    "\n",
    "pipeline = Pipeline(steps=steps) ### object\n",
    "\n",
    "pipeline.fit(X_train, y_train) ## training\n",
    "\n",
    "y_hat = pipeline.predict(X_test) ## prediction\n",
    "print('Validation set: confusion matrix with random-over-sampling')\n",
    "cm = confusion_matrix(y_test, y_hat, labels=[0,1])\n",
    "annot_kws = {\"ha\": 'left',\"va\": 'top'}\n",
    "sns.heatmap(cm, annot=True, center = True,fmt=\"d\",linewidths=0.5,cmap=\"Greens\",vmin=0,vmax = cm.max(),annot_kws=annot_kws)\n",
    "\n",
    "print('Training AUC Score: {}'.format(round(roc_auc_score(y_train, pipeline.predict(X_train))*100,4)))\n",
    "print('validation AUC Score: {}'.format(round(roc_auc_score(y_test, y_hat)*100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values to evaluate\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "k_values = [1, 2, 3, 4, 5, 6, 7]\n",
    "for k in k_values:\n",
    "\t# define pipeline\n",
    "\tmodel = RandomForestClassifier()\n",
    "\tover  = SMOTE(sampling_strategy=0.1, k_neighbors=k)\n",
    "\tunder = RandomUnderSampler(sampling_strategy=0.5)\n",
    "\tsteps = [('over', over), ('under', under), ('model', model)]\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\t# evaluate pipeline\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\tscores = cross_val_score(pipeline, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "\tscore = np.mean(scores)\n",
    "\tprint('> k=%d, Mean ROC AUC: %.3f' % (k, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier usin GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Randomforest classifier\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_train, model_rf.predict(X_train)))\n",
    "print(accuracy_score(y_test, model_rf.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Randomforestclassifier using Gridsearch cv\n",
    "\n",
    "param = {'max_depth':[1,2,3,4],\n",
    "        'max_features':['auto','sqrt'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_r = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param, scoring='accuracy', cv = 3,\n",
    "                 n_jobs= -1)\n",
    "#estimator cab be LR, decision tree etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gs_r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a750e23029f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgs_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_r\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgs_r\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgs_r\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gs_r' is not defined"
     ]
    }
   ],
   "source": [
    "gs_r = gs_r.fit(X_train, y_train)\n",
    "\n",
    "gs_r.best_score_\n",
    "\n",
    "gs_r.best_params_\n",
    "\n",
    "print(accuracy_score(y_train, gs_r.predict(X_train)))\n",
    "print(accuracy_score(y_test, gs_r.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest classifier Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth':[1,2,3,4],\n",
    "        'max_features':['auto','sqrt'] }\n",
    "\n",
    "gs_r = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param, scoring='accuracy', cv = 10,\n",
    "                 n_jobs= 2)\n",
    "\n",
    "gs_r = gs_r.fit(X_train, y_train)\n",
    "print(accuracy_score(y_train, gs_r.predict(X_train)))\n",
    "print(accuracy_score(y_test, gs_r.predict(X_test)))\n",
    "\n",
    "#prediction on the test.csv\n",
    "y_testsetRF = gs_r.predict(X_test_)\n",
    "print(X_test_.shape)\n",
    "print(y_testsetRF.shape)\n",
    "test2.shape\n",
    "\n",
    "test2['Survived'] = y_testsetRF\n",
    "test2.head()\n",
    "\n",
    "### save dataframe as csv file\n",
    "test2[['PassengerId','Survived']].to_csv('submission.csv', index = False)\n",
    "\n",
    "pd.DataFrame(gs_r.cv_results_).sort_values(by = ['mean_test_score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with depth = 2\n",
    "model_dt_2 = DecisionTreeClassifier(random_state=1, max_depth=2)\n",
    "model_dt_2.fit(X_train, y_train)\n",
    "model_dt_2_score_train = model_dt_2.score(X_train, y_train)\n",
    "print(\"Training score: \",model_dt_2_score_train)\n",
    "model_dt_2_score_test = model_dt_2.score(X_test, y_test)\n",
    "print(\"Testing score: \",model_dt_2_score_test)\n",
    "#y_pred_dt = model_dt_2.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "\n",
    "#Decision tree\n",
    "\n",
    "model_dt = DecisionTreeClassifier(max_depth = 8, criterion =\"entropy\")\n",
    "model_dt.fit(X_train, y_train)\n",
    "y_pred_dt = model_dt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "\n",
    "predictions = model_dt.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print (model_dt.score(X_test, y_test))\n",
    "y_actual_result = y_test\n",
    "\n",
    "\n",
    "print (confusion_matrix(y_test, predictions))\n",
    "\n",
    "\n",
    "\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# area under curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_dt)\n",
    "roc_auc_dt = auc(fpr_dt, tpr_dt)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "lw = 2\n",
    "plt.plot(fpr_dt, tpr_dt, color='green',\n",
    "         lw=lw, label='Decision Tree(AUC = %0.2f)' % roc_auc_dt)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Area Under Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "svc=SVC() #Default hyperparameters\n",
    "svc.fit(X_train,y_train)\n",
    "y_pred=svc.predict(X_test)\n",
    "print('Accuracy Score:')\n",
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "\n",
    "\n",
    "\n",
    "# linear kernal\n",
    "svc=SVC(kernel='linear')\n",
    "'''# polynomial kernal\n",
    "svc=SVC(kernel='poly')\n",
    "\n",
    "# radial kernal\n",
    "svc=SVC(kernel='rbf')\n",
    "'''\n",
    "svc.fit(X_train,y_train)\n",
    "y_pred=svc.predict(X_test)\n",
    "print('Accuracy Score:')\n",
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
